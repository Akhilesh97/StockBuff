{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a7aec8",
   "metadata": {},
   "source": [
    "# Financial News Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a487cea",
   "metadata": {},
   "source": [
    "### Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb61d4",
   "metadata": {},
   "source": [
    "- No need for lemmatizing/stemming\n",
    "- No need for domain stop-words\n",
    "- Used SentBert for vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c6115",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd23410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK loaded.\n",
      "Spacy loaded.\n",
      "PyTorch loaded.\n"
     ]
    }
   ],
   "source": [
    "'''Kernel Python Version 3.6.10 '''\n",
    "\n",
    "# Standard libs\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "import io\n",
    "from io import StringIO\n",
    "import inspect\n",
    "import shutil\n",
    "import ast\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import traceback\n",
    "import multiprocessing\n",
    "import requests\n",
    "import logging\n",
    "import math\n",
    "import pytz\n",
    "from itertools import chain\n",
    "from string import Template\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "import base64\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import tempfile\n",
    "import jsonschema\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple, NamedTuple, Union\n",
    "from functools import wraps\n",
    "\n",
    "# graph\n",
    "import networkx as nx\n",
    "\n",
    "# Required pkgs\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import tqdm\n",
    "\n",
    "# General text correction - fit text for you (ftfy) and others\n",
    "import ftfy\n",
    "from fuzzywuzzy import fuzz\n",
    "from wordcloud import WordCloud\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, ADASYN\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, jaccard_score, silhouette_score, homogeneity_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# scipy\n",
    "from scipy import spatial, sparse\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "from scipy.spatial.distance import euclidean, jensenshannon, cosine, cdist\n",
    "from scipy.io import mmwrite, mmread\n",
    "from scipy.stats import entropy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, fcluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# sparse_dot_topn: matrix multiplier\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases, Word2Vec, KeyedVectors, FastText, LdaModel\n",
    "from gensim import utils\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import gensim.downloader as api\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#nltk_model_data_path = \"/someppath/\"\n",
    "#nltk.data.path.append(nltk_model_data_path)\n",
    "from nltk import FreqDist, tokenize, sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print(\"NLTK loaded.\")\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "# spacy_model_data_path = \"/Users/pranjalpathak/opt/anaconda3/envs/Python_3.6/lib/python3.6/site-packages/en_core_web_lg/en_core_web_lg-2.2.5\"\n",
    "nlp = spacy.load('en_core_web_lg')  # disabling: nlp = spacy.load(spacy_data_path, disable=['ner'])\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "print(\"Spacy loaded.\")\n",
    "\n",
    "# TF & Keras\n",
    "# import tensorflow as tf\n",
    "# from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import CustomObjectScope\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "# from keras.engine.topology import Layer\n",
    "# from keras import backend as K\n",
    "# from keras import initializers as initializers, regularizers, constraints, optimizers\n",
    "# from keras.layers import *\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.layers.recurrent import LSTM\n",
    "# # from keras.layers.core import Input, Dense, Activation\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "# from keras.models import Sequential, Model, load_model\n",
    "# import tensorflow_hub as hub\n",
    "# print(\"TensorFlow loaded.\")\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelWithLMHead\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "print(\"PyTorch loaded.\")\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import offline\n",
    "%matplotlib inline\n",
    "\n",
    "# Theme settings\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "sns.set_context('talk')\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.set_style(\"darkgrid\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee27d33",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "023cc754",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(\"../\")\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "output_dir = os.path.join(root_dir, \"outputs\")\n",
    "\n",
    "PATH_RES_DIR = \"/Volumes/Local Drive/WORK/Machine Learning/!!! Resources !!!!/NLP Resource Files\"\n",
    "\n",
    "PATH_BERT_MODEL = os.path.join(os.path.join(root_dir, \"models\"), \"finbert_v1\")\n",
    "\n",
    "    \n",
    "# bert_model_fp = \"/v/region/na/appl/mswm/ainlp/data/ainlp_dev/Pretrained_Models/sentence-transformers-models/all-distilroberta-v1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f2e22",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2167fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(data_dir, \"Sample_News.csv\"), index_col=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df761ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_txt = \"title\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214b2f1",
   "metadata": {},
   "source": [
    "## Preprocessinng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9f3d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessText:\n",
    "    \n",
    "    def __init__(self, resources_dir_path, custom_vocab=[], do_lemma=False):\n",
    "        self.stopwords_file = os.path.join(resources_dir_path, \"stopwords.txt\")\n",
    "        self.special_stopwords_file = os.path.join(resources_dir_path, \"special_stopwords.txt\")\n",
    "        self.special_characters_file = os.path.join(resources_dir_path, \"special_characters.txt\")\n",
    "        self.contractions_file = os.path.join(resources_dir_path, \"contractions.json\")\n",
    "        self.chatwords_file = os.path.join(resources_dir_path, \"chatwords.txt\")\n",
    "        self.emoticons_file = os.path.join(resources_dir_path, \"emoticons.json\")\n",
    "        self.greeting_file = os.path.join(resources_dir_path, \"greeting_words.txt\")\n",
    "        self.signature_file = os.path.join(resources_dir_path, \"signature_words.txt\")\n",
    "        self.preserve_key = \"<$>\" # preserve special vocab\n",
    "        self.vocab_list = custom_vocab\n",
    "        self.preseve = True if len(custom_vocab) > 0 else False\n",
    "        self.load_resources()\n",
    "        self.do_lemma = do_lemma\n",
    "        return\n",
    "    \n",
    "    def load_resources(self):\n",
    "        \n",
    "        ### Build Vocab Model --> Words to keep\n",
    "        self.vocab_list = set(map(str.lower, self.vocab_list))\n",
    "        self.vocab_dict = {w: self.preserve_key.join(w.split()) for w in self.vocab_list}\n",
    "        self.re_retain_words = re.compile('|'.join(sorted(map(re.escape, self.vocab_dict), key=len, reverse=True)))\n",
    "        \n",
    "        ### Build Stopwords Model --> Words to drop/delete\n",
    "        with open(self.stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords = [x.rstrip() for x in f.readlines()]\n",
    "        with open(self.special_stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        with open(self.special_characters_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        self.stopwords = list(sorted(set(self.stopwords).difference(self.vocab_list)))\n",
    "\n",
    "        ### Build Contractions\n",
    "        with open(self.contractions_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.contractions = dict(json.load(f))\n",
    "        \n",
    "        ### Build Chat-words\n",
    "        with open(self.chatwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.chat_words_map_dict, self.chat_words_list = {}, []\n",
    "            chat_words = [x.rstrip() for x in f.readlines()]\n",
    "            for line in chat_words:\n",
    "                cw = line.split(\"=\")[0]\n",
    "                cw_expanded = line.split(\"=\")[1]\n",
    "                self.chat_words_list.append(cw)\n",
    "                self.chat_words_map_dict[cw] = cw_expanded\n",
    "            self.chat_words_list = set(self.chat_words_list)\n",
    "        \n",
    "        ### Bukd social markups\n",
    "        # emoticons\n",
    "        with open(self.emoticons_file, \"r\") as f:\n",
    "            self.emoticons = re.compile(u'(' + u'|'.join(k for k in json.load(f)) + u')')\n",
    "        # emojis\n",
    "        self.emojis = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        # greeting\n",
    "        with open(self.greeting_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.greeting_words = [x.rstrip() for x in f.readlines()]\n",
    "        # signature\n",
    "        with open(self.signature_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.signature_words = [x.rstrip() for x in f.readlines()]\n",
    "        # spell-corrector\n",
    "        self.spell_checker = SpellChecker()   \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def reserve_keywords_from_cleaning(self, text, reset=False):\n",
    "        \"\"\" \n",
    "        Finds common words from a user-provided list of special keywords to preserve them from \n",
    "        cleaning steps. Identifies every special keyword and joins them using `self.preserve_key` during the \n",
    "        cleaning steps, and later resets it back to original word in the end.\n",
    "        \"\"\"\n",
    "        if reset is False:\n",
    "            # compile using a dict of words and their expansions, and sub them if found!\n",
    "            match_and_sub = self.re_retain_words.sub(lambda x: self.vocab_dict[x.string[x.start():x.end()]], text)\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", match_and_sub).strip()\n",
    "        else:\n",
    "            # reverse the change! - use this at the end of preprocessing\n",
    "            text = text.replace(self.preserve_key, \" \")\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", text).strip()\n",
    "\n",
    "\n",
    "    def basic_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in input_sentences:\n",
    "            sent = str(sent).strip()\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <â€¦> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0â€“9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?Â«Â»\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).strip()\n",
    "            cleaned_sentences.append(sent)\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def deep_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in input_sentences:\n",
    "            # normalize text to \"utf-8\" encoding\n",
    "            sent = unicodedata.normalize('NFKD', str(sent)).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # lowercasing\n",
    "            sent = str(sent).strip().lower()\n",
    "\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            #\n",
    "            # *** Mark important keywords such as: Domain specific, Question words(wh-words), etc, using \n",
    "            # \"self.vocab_list\". Words from this list if found in any input sentence shall be joined using \n",
    "            # a key (self.preserve_key) during pre-processing step, and later un-joined to retain them.\n",
    "            #\n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=False)\n",
    "            #\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "\n",
    "            # remove Emojis\n",
    "            sent = self.emojis.sub(r'', sent)\n",
    "            # remove emoticons\n",
    "            sent = self.emoticons.sub(r'', sent)\n",
    "            # remove common chat-words\n",
    "            sent = \" \".join([self.chat_words_map_dict[w.upper()] if w.upper() in self.chat_words_list else w for w in sent.split()])\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <â€¦> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0â€“9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?Â«Â»\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "            #\n",
    "            # removing punctuations ðŸ”¥ðŸ”¥\n",
    "            # *** disable them, when sentence structure needs to be retained ***\n",
    "            sent = re.sub(r\"[\\$|\\#\\@\\*\\%]+\\d+[\\$|\\#\\@\\*\\%]+\", \" \", sent)\n",
    "            sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n",
    "            sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n",
    "            sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n",
    "            sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n",
    "            sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n",
    "            sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n",
    "            sent = re.sub(r\"[\\/,\\@,\\#,\\\\,\\{,\\},\\(,\\),\\[,\\],\\$,\\%,\\^,\\&,\\*,\\<,\\>]\", \" \", sent)\n",
    "            sent = re.sub(r\"[\\,,\\;,\\:,\\-]\", \" \", sent)      # main puncts\n",
    "            \n",
    "            # remove sentence de-limitters ðŸ”¥ðŸ”¥\n",
    "            # *** disable them, when sentence boundary/ending is important ***\n",
    "            # sent = re.sub(r\"[\\!,\\?,\\.]\", \" \", sent)\n",
    "\n",
    "            # keep only text & numbers ðŸ”¥ðŸ”¥\n",
    "            # *** enable them, when only text and numbers matter! *** \n",
    "            # sent = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\\\|\\/|\\||\\{|\\}|\\[|\\]\\(|\\)]+\", \" \", re.sub(r\"[^A-z0-9]\", \" \", str(sent))))\n",
    "            \n",
    "            # correct spelling mistakes ðŸ”¥ðŸ”¥\n",
    "            # *** enable them when english spelling mistakes matter *** \n",
    "            # sent = \" \".join([self.spell_checker.correction(w) if w in self.spell_checker.unknown(sent.split()) else w for w in sent.split()])\n",
    "            #\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "\n",
    "            # Remove stopwords\n",
    "            sent = \" \".join(token.text for token in nlp(sent) if token.text not in self.stopwords and \n",
    "                                                                 token.lemma_ not in self.stopwords)\n",
    "            # Lemmatize\n",
    "            if self.do_lemma:\n",
    "                sent = \" \".join(token.lemma_ for token in nlp(sent))\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).lower().strip()\n",
    "\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            #\n",
    "            # *** Reverse the custom joining now to un-join the special words found!\n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=True)\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "\n",
    "            cleaned_sentences.append(sent.strip().lower())\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def spacy_get_pos_list(self, results):\n",
    "        word_list, pos_list, lemma_list, ner_list, start_end_list = [], [], [], [], []\n",
    "        indices = results['sentences']\n",
    "        for line in indices:\n",
    "            tokens = line['tokens']\n",
    "            for token in tokens:\n",
    "                # (1). save tokens\n",
    "                word_list.append(token['word'])\n",
    "                # (2). save pos\n",
    "                pos_list.append(token['pos'])\n",
    "                # (3). save lemmas\n",
    "                lemma = token['lemma'].lower()\n",
    "                if lemma in self.stopwords: continue\n",
    "                lemma_list.append(lemma)\n",
    "                # (4). save NER\n",
    "                ner_list.append(token['ner'])\n",
    "                # (5). save start\n",
    "                start_end_list.append(str(token['characterOffsetBegin']) + \"_\" + str(token['characterOffsetEnd']))\n",
    "        output = {\"word_list\": word_list, \n",
    "                  \"lemma_list\": lemma_list, \n",
    "                  \"token_start_end_list\": start_end_list,\n",
    "                  \"pos_list\": pos_list, \"ner_list\": ner_list}\n",
    "        return output\n",
    "\n",
    "    def spacy_generate_features(self, doc, operations='tokenize,ssplit,pos,lemma,ner'):\n",
    "        \"\"\"\n",
    "        Spacy nlp pipeline to generate features such as pos, tokens, ner, dependency. Accepts doc=nlp(text)\n",
    "        \"\"\"\n",
    "        # spacy doc\n",
    "        doc_json = doc.to_json()  # Includes all operations given by spacy pipeline\n",
    "\n",
    "        # Get text\n",
    "        text = doc_json['text']\n",
    "\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        # 1. Extract Entity List\n",
    "        entity_list = doc_json[\"ents\"]\n",
    "\n",
    "        # 2. Create token lib\n",
    "        token_lib = {token[\"id\"]: token for token in doc_json[\"tokens\"]}\n",
    "\n",
    "        # init output json\n",
    "        output_json = {}\n",
    "        output_json[\"sentences\"] = []\n",
    "\n",
    "        # Perform spacy operations on each sent in text\n",
    "        for i, sentence in enumerate(doc_json[\"sents\"]):\n",
    "            # init parsers\n",
    "            parse = \"\"\n",
    "            basicDependencies = []\n",
    "            enhancedDependencies = []\n",
    "            enhancedPlusPlusDependencies = []\n",
    "\n",
    "            # init output json\n",
    "            out_sentence = {\"index\": i, \"line\": 1, \"tokens\": []}\n",
    "            output_json[\"sentences\"].append(out_sentence)\n",
    "\n",
    "            # 3. Split sentences by indices(i), add labels (pos, ner, dep, etc.)\n",
    "            for token in doc_json[\"tokens\"]:\n",
    "\n",
    "                if sentence[\"start\"] <= token[\"start\"] and token[\"end\"] <= sentence[\"end\"]:\n",
    "                    \n",
    "                    # >>> Extract Entity label\n",
    "                    ner = \"O\"\n",
    "                    for entity in entity_list:\n",
    "                        if entity[\"start\"] <= token[\"start\"] and token[\"end\"] <= entity[\"end\"]:\n",
    "                            ner = entity[\"label\"]\n",
    "\n",
    "                    # >>> Extract dependency info\n",
    "                    dep = token[\"dep\"]\n",
    "                    governor = 0 if token[\"head\"] == token[\"id\"] else (token[\"head\"] + 1)  # CoreNLP index = pipeline index +1\n",
    "                    governorGloss = \"ROOT\" if token[\"head\"] == token[\"id\"] else text[token_lib[token[\"head\"]][\"start\"]:\n",
    "                                                                                     token_lib[token[\"head\"]][\"end\"]]\n",
    "                    dependent = token[\"id\"] + 1\n",
    "                    dependentGloss = text[token[\"start\"]:token[\"end\"]]\n",
    "\n",
    "                    # >>> Extract lemma\n",
    "                    lemma = doc[token[\"id\"]].lemma_\n",
    "\n",
    "                    # 4. Add dependencies\n",
    "                    basicDependencies.append({\"dep\": dep,\n",
    "                                              \"governor\": governor,\n",
    "                                              \"governorGloss\": governorGloss,\n",
    "                                              \"dependent\": dependent,\n",
    "                                              \"dependentGloss\": dependentGloss})\n",
    "                    # 5. Add tokens\n",
    "                    out_token = {\"index\": token[\"id\"] + 1,\n",
    "                                 \"word\": dependentGloss,\n",
    "                                 \"originalText\": dependentGloss,\n",
    "                                 \"characterOffsetBegin\": token[\"start\"],\n",
    "                                 \"characterOffsetEnd\": token[\"end\"]}\n",
    "\n",
    "                    # 6. Add lemmas\n",
    "                    if \"lemma\" in operations:\n",
    "                        out_token[\"lemma\"] = lemma\n",
    "\n",
    "                    # 7. Add POS tagging\n",
    "                    if \"pos\" in operations:\n",
    "                        out_token[\"pos\"] = token[\"tag\"]\n",
    "\n",
    "                    # 8. Add NER\n",
    "                    if \"ner\" in operations:\n",
    "                        out_token[\"ner\"] = ner\n",
    "\n",
    "                    # Update output json\n",
    "                    out_sentence[\"tokens\"].append(out_token)\n",
    "\n",
    "            # 9. Add dependencies operation\n",
    "            if \"parse\" in operations:\n",
    "                out_sentence[\"parse\"] = parse\n",
    "                out_sentence[\"basicDependencies\"] = basicDependencies\n",
    "                out_sentence[\"enhancedDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "                out_sentence[\"enhancedPlusPlusDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        return output_json\n",
    "    \n",
    "    def spacy_clean(self, input_sentences):\n",
    "        batch_size = min(int(np.ceil(len(input_sentences)/100)), 500)\n",
    "        \n",
    "        # Part 1: generate spacy textual features (pos, ner, lemma, dependencies)\n",
    "        sentences = [self.spacy_generate_features(doc) for doc in nlp.pipe(input_sentences, batch_size=batch_size, n_threads=-1)]\n",
    "        \n",
    "        # Part 2: collect all the features for each sentence\n",
    "        spacy_sentences = [self.spacy_get_pos_list(sent) for sent in sentences]\n",
    "\n",
    "        return spacy_sentences\n",
    "\n",
    "\n",
    "    ## MAIN ##\n",
    "    def run_pipeline(self, sentences, operation):\n",
    "        \"\"\"\n",
    "        Main module to execute pipeline. Accepts list of strings, and desired operation.\n",
    "        \"\"\"\n",
    "        if operation==\"\":\n",
    "            raise Exception(\"Please pass a cleaning type - `basic`, `deep` or `spacy` !!\")\n",
    "\n",
    "        # run basic cleaning\n",
    "        if \"basic\" == operation.lower(): \n",
    "            return self.basic_clean(sentences)\n",
    "\n",
    "        # run deep cleaning\n",
    "        if \"deep\" == operation.lower(): \n",
    "            return self.deep_clean(sentences)\n",
    "\n",
    "        # run spacy pipeline\n",
    "        if \"spacy\" == operation.lower(): \n",
    "            return self.spacy_clean(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee97bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CUSTOM VOCABULARY\n",
    "- List of words you wish to mark and retain them across the preprocessing steps. \n",
    "- Example, task-specific, domain-specific keywords.\n",
    "\"\"\"\n",
    "custom_vocab = [\"google\", \"goog\", \"alphabet\", \"googlee\", \"netflix\", \"netflx\", \"amazon\", \"amz\", \n",
    "                \"apple\", \"aple\", \"aws\", \"iphone\", \"mac\", \"ipad\"]\n",
    "\n",
    "\"\"\"\n",
    "LEMMATIZER\n",
    "- Truncate words to their root-known-word form, stripping off their adjectives, verbs, etc.\n",
    "- Example: \"running\" becomes \"run\", \"is\" becomes \"be\"\n",
    "\"\"\"\n",
    "do_lemmatizing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7ae905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "\n",
    "resources_dir_path = PATH_RES_DIR\n",
    "preprocessText_obj = preprocessText(resources_dir_path, custom_vocab, do_lemmatizing)\n",
    "\n",
    "def cleaning(data, text_col):\n",
    "    data[\"Basic_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"basic\")\n",
    "    data[\"Deep_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"deep\")\n",
    "    data[\"Spacy_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"spacy\")\n",
    "    return data\n",
    "\n",
    "\n",
    "## SAMPLE\n",
    "# df = cleaning(df, <_TEXT_COLUMN_>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e33b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaning(data, col_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0dcf3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Vectorization using sentence-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "915ed0e2",
   "metadata": {
    "code_folding": [
     2,
     11,
     33,
     38
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Volumes/Local Drive/CU Boulder/Google Drive/My Drive/CU Boulder/Academics/2. CSCI 5502-002 - Data Mining/Course Project/StockBuff-main/models/finbert_v1 were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert Model 'finbert_v1' loaded.\n"
     ]
    }
   ],
   "source": [
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, tokenizer, model, max_length=128, embedding_func: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_func = embedding_func\n",
    "        if self.embedding_func is None:\n",
    "            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        # Mean Pooling - Take attention mask into account for correct averaging\n",
    "        def mean_pooling(model_output, attention_mask):\n",
    "            token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return sum_embeddings / sum_mask\n",
    "\n",
    "        # Tokenize the text with the provided tokenizer\n",
    "        sentence_embeddings = tokenizer(text, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        # with torch.no_grad():\n",
    "        #    model_output = self.model(**encoded_input)\n",
    "\n",
    "        # Perform mean pooling\n",
    "        # sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        # bert takes in a batch so we need to unsqueeze the rows\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def transform(self, text: List[str]):\n",
    "        if isinstance(text, pd.Series):\n",
    "            text = text.tolist()\n",
    "        return self._tokenize(text)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"No fitting required so we just return ourselves. For fine-tuning, refer to shared gpu-code!\"\"\"\n",
    "        return self\n",
    "    \n",
    "# BERT Model\n",
    "bert_model_fp = PATH_BERT_MODEL\n",
    "\n",
    "# load tokenizer, model classes\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_fp)\n",
    "model_bert = AutoModel.from_pretrained(bert_model_fp)\n",
    "classify_model_bert = AutoModelForSequenceClassification.from_pretrained(bert_model_fp)\n",
    "\n",
    "# load vectorizer\n",
    "bert_vectorizer = BertTransformer(tokenizer, model_bert, embedding_func=lambda x: x[0][:, 0, :].squeeze())\n",
    "print(\"Bert Model '%s' loaded.\" % ntpath.basename(bert_model_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1fd5a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments(df_text_col):\n",
    "    \n",
    "    # vectorize\n",
    "    sentence_embbeds = bert_vectorizer.transform(df_text_col)\n",
    "    \n",
    "    # AutoModelForSequenceClassification(FinBert) outputs sentiments [Pos, Neg, Neu] absolute scores\n",
    "    output = classify_model_bert(**sentence_embbeds)\n",
    "    \n",
    "    # need to predict probability that sums to 1.0, SUM(pos_prob + neg_prob + neu_prob) = 1\n",
    "    scaled_output = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "    \n",
    "    senti_pos_scores = scaled_output[:, 0].tolist()\n",
    "    senti_neg_scores = scaled_output[:, 1].tolist()\n",
    "    senti_neu_scores = scaled_output[:, 2].tolist()\n",
    "    return senti_pos_scores, senti_neg_scores, senti_neu_scores\n",
    "\n",
    "# getting sentiments\n",
    "df['Positive'], df['Negative'], df['Neutral'] = get_sentiments(df['Deep_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "50924bb9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>symbols</th>\n",
       "      <th>tags</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Basic_title</th>\n",
       "      <th>Deep_title</th>\n",
       "      <th>Spacy_title</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-01T23:04:15+00:00</td>\n",
       "      <td>SoftBank-Backed Mapbox Names New CEO to Go Aft...</td>\n",
       "      <td>(Bloomberg) -- Mapbox Inc., which makes mappin...</td>\n",
       "      <td>https://finance.yahoo.com/news/softbank-backed...</td>\n",
       "      <td>['AAPL.US', 'AMZN.US', 'AONE-UN.US', 'AONE.US'...</td>\n",
       "      <td>['BLOOMBERG', 'BY\\xa0INSTACART INC', 'CHIEF EX...</td>\n",
       "      <td>{'polarity': 0.974, 'neg': 0, 'neu': 0.934, 'p...</td>\n",
       "      <td>SoftBank-Backed Mapbox Names New CEO to Go Aft...</td>\n",
       "      <td>softbank backed mapbox names new ceo go carmakers</td>\n",
       "      <td>{'word_list': ['SoftBank', '-', 'Backed', 'Map...</td>\n",
       "      <td>0.178975</td>\n",
       "      <td>0.087238</td>\n",
       "      <td>0.733787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-01T22:05:12+00:00</td>\n",
       "      <td>Dow Jones Rips 600 Points Higher, Led By Boein...</td>\n",
       "      <td>The Dow Jones rallied sharply along with the o...</td>\n",
       "      <td>https://finance.yahoo.com/m/ae1cdff5-1fcf-3c5e...</td>\n",
       "      <td>['AAPL.US', 'BA.US', 'JNJ.US', 'SMG.US', 'ZM.US']</td>\n",
       "      <td>['APPLE STOCK', 'DOW JONES', 'NASDAQ', 'NASDAQ...</td>\n",
       "      <td>{'polarity': 0.494, 'neg': 0, 'neu': 0.887, 'p...</td>\n",
       "      <td>Dow Jones Rips 600 Points Higher, Led By Boein...</td>\n",
       "      <td>dow jones rips 600 points higher led by boeing...</td>\n",
       "      <td>{'word_list': ['Dow', 'Jones', 'Rips', '600', ...</td>\n",
       "      <td>0.311447</td>\n",
       "      <td>0.477432</td>\n",
       "      <td>0.211121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-01T20:20:54+00:00</td>\n",
       "      <td>Dow Jones Rises Over 700 Points As Indexes Rec...</td>\n",
       "      <td>The Dow Jones Industrial Average held strong g...</td>\n",
       "      <td>https://finance.yahoo.com/m/fa4aff04-adc3-352a...</td>\n",
       "      <td>['AAPL.US', 'BA.US', 'INTC.US', 'JNJ.US', 'TER...</td>\n",
       "      <td>['DOW JONES', 'STOCK MARKET', 'THE DOW', 'U.S....</td>\n",
       "      <td>{'polarity': 0.896, 'neg': 0, 'neu': 0.59, 'po...</td>\n",
       "      <td>Dow Jones Rises Over 700 Points As Indexes Rec...</td>\n",
       "      <td>dow jones rises 700 points indexes recover las...</td>\n",
       "      <td>{'word_list': ['Dow', 'Jones', 'Rises', 'Over'...</td>\n",
       "      <td>0.112521</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.267761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-01T20:20:54+00:00</td>\n",
       "      <td>Dow Jones Rises Over 700 Points As Indexes Rec...</td>\n",
       "      <td>The Dow Jones Industrial Average held strong g...</td>\n",
       "      <td>https://finance.yahoo.com/m/fa4aff04-adc3-352a...</td>\n",
       "      <td>['AAPL.US', 'BA.US', 'INTC.US', 'JNJ.US', 'TER...</td>\n",
       "      <td>['DOW JONES', 'STOCK MARKET', 'THE DOW', 'U.S....</td>\n",
       "      <td>{'polarity': 0.896, 'neg': 0, 'neu': 0.59, 'po...</td>\n",
       "      <td>Dow Jones Rises Over 700 Points As Indexes Rec...</td>\n",
       "      <td>dow jones rises 700 points indexes recover las...</td>\n",
       "      <td>{'word_list': ['Dow', 'Jones', 'Rises', 'Over'...</td>\n",
       "      <td>0.112521</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.267761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-01T19:01:11+00:00</td>\n",
       "      <td>Warren Buffett signals more stock buybacks com...</td>\n",
       "      <td>Jonathan Boyar, Boyar Asset Management Princip...</td>\n",
       "      <td>https://finance.yahoo.com/video/warren-buffett...</td>\n",
       "      <td>['AAPL.US', 'AXP.US', 'BAC-PP.US', 'BAC.US', '...</td>\n",
       "      <td>['BOYAR', 'JONATHAN BOYAR', 'KRISTIN MYERS', '...</td>\n",
       "      <td>{'polarity': 0.999, 'neg': 0.04, 'neu': 0.821,...</td>\n",
       "      <td>Warren Buffett signals more stock buybacks com...</td>\n",
       "      <td>warren buffett signals stock buybacks coming y...</td>\n",
       "      <td>{'word_list': ['Warren', 'Buffett', 'signals',...</td>\n",
       "      <td>0.321768</td>\n",
       "      <td>0.145025</td>\n",
       "      <td>0.533206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>2018-01-31T12:55:00+00:00</td>\n",
       "      <td>Investor Expectations to Drive Momentum within...</td>\n",
       "      <td>NEW YORK, Jan.  31, 2018  (GLOBE NEWSWIRE) -- ...</td>\n",
       "      <td>https://www.globenewswire.com/news-release/201...</td>\n",
       "      <td>['AAPL.US', 'AVHI.US', 'FARM.US', 'GM.US', 'SG...</td>\n",
       "      <td>['FUNDAMENTAL MARKETS']</td>\n",
       "      <td>{'polarity': 0.995, 'neg': 0.009, 'neu': 0.937...</td>\n",
       "      <td>Investor Expectations to Drive Momentum within...</td>\n",
       "      <td>investor expectations drive momentum within co...</td>\n",
       "      <td>{'word_list': ['Investor', 'Expectations', 'to...</td>\n",
       "      <td>0.073805</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.920796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>2017-11-30T14:27:00+00:00</td>\n",
       "      <td>BioTelemetry, Inc. Enters Agreement to Provide...</td>\n",
       "      <td>MALVERN, Pa., Nov.  30, 2017  (GLOBE NEWSWIRE)...</td>\n",
       "      <td>https://www.globenewswire.com/news-release/201...</td>\n",
       "      <td>['AAPL.US', 'BEAT.US']</td>\n",
       "      <td>['BIOTELEMETRY', 'INC', 'NASDAQ:BEAT']</td>\n",
       "      <td>{'polarity': 0.989, 'neg': 0.021, 'neu': 0.804...</td>\n",
       "      <td>BioTelemetry, Inc. Enters Agreement to Provide...</td>\n",
       "      <td>biotelemetry inc enters agreement provide card...</td>\n",
       "      <td>{'word_list': ['BioTelemetry', ',', 'Inc.', 'E...</td>\n",
       "      <td>0.657360</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.334330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>2017-11-27T13:00:00+00:00</td>\n",
       "      <td>Factors of Influence in 2018, Key Indicators a...</td>\n",
       "      <td>NEW YORK, Nov.  27, 2017  (GLOBE NEWSWIRE) -- ...</td>\n",
       "      <td>https://www.globenewswire.com/news-release/201...</td>\n",
       "      <td>['AAPL.US', 'CSCO.US', 'GD.US', 'HPE.US', 'NVD...</td>\n",
       "      <td>['FUNDAMENTAL MARKETS']</td>\n",
       "      <td>{'polarity': 0.997, 'neg': 0.008, 'neu': 0.926...</td>\n",
       "      <td>Factors of Influence in 2018, Key Indicators a...</td>\n",
       "      <td>factors influence 2018 key indicators opportun...</td>\n",
       "      <td>{'word_list': ['Factors', 'of', 'Influence', '...</td>\n",
       "      <td>0.055207</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.935690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>2017-10-05T15:58:00+00:00</td>\n",
       "      <td>New Research: Key Drivers of Growth for Micros...</td>\n",
       "      <td>NEW YORK, Oct.  05, 2017  (GLOBE NEWSWIRE) -- ...</td>\n",
       "      <td>https://www.globenewswire.com/news-release/201...</td>\n",
       "      <td>['AAPL.US', 'AMZN.US', 'INTC.US', 'MSFT.US', '...</td>\n",
       "      <td>['FUNDAMENTAL MARKETS']</td>\n",
       "      <td>{'polarity': 0.997, 'neg': 0.008, 'neu': 0.925...</td>\n",
       "      <td>New Research: Key Drivers of Growth for Micros...</td>\n",
       "      <td>new research key drivers growth microsoft appl...</td>\n",
       "      <td>{'word_list': ['New', 'Research', ':', 'Key', ...</td>\n",
       "      <td>0.034298</td>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.954141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>2016-02-19T15:05:00+00:00</td>\n",
       "      <td>Payment Data Systems Announces Apple Pay Suppo...</td>\n",
       "      <td>SAN ANTONIO, Feb.  19, 2016  (GLOBE NEWSWIRE) ...</td>\n",
       "      <td>https://www.globenewswire.com/news-release/201...</td>\n",
       "      <td>['AAPL.US', 'PYDS.US', 'USIO.US']</td>\n",
       "      <td>['AKIMBO', 'AKIMBO CARD', 'APPLE', 'APPLE PAY'...</td>\n",
       "      <td>{'polarity': 0.994, 'neg': 0.023, 'neu': 0.869...</td>\n",
       "      <td>Payment Data Systems Announces Apple Pay Suppo...</td>\n",
       "      <td>payment data systems announces apple pay suppo...</td>\n",
       "      <td>{'word_list': ['Payment', 'Data', 'Systems', '...</td>\n",
       "      <td>0.397003</td>\n",
       "      <td>0.104784</td>\n",
       "      <td>0.498213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>645 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date  \\\n",
       "0    2021-03-01T23:04:15+00:00   \n",
       "1    2021-03-01T22:05:12+00:00   \n",
       "2    2021-03-01T20:20:54+00:00   \n",
       "3    2021-03-01T20:20:54+00:00   \n",
       "4    2021-03-01T19:01:11+00:00   \n",
       "..                         ...   \n",
       "640  2018-01-31T12:55:00+00:00   \n",
       "641  2017-11-30T14:27:00+00:00   \n",
       "642  2017-11-27T13:00:00+00:00   \n",
       "643  2017-10-05T15:58:00+00:00   \n",
       "644  2016-02-19T15:05:00+00:00   \n",
       "\n",
       "                                                 title  \\\n",
       "0    SoftBank-Backed Mapbox Names New CEO to Go Aft...   \n",
       "1    Dow Jones Rips 600 Points Higher, Led By Boein...   \n",
       "2    Dow Jones Rises Over 700 Points As Indexes Rec...   \n",
       "3    Dow Jones Rises Over 700 Points As Indexes Rec...   \n",
       "4    Warren Buffett signals more stock buybacks com...   \n",
       "..                                                 ...   \n",
       "640  Investor Expectations to Drive Momentum within...   \n",
       "641  BioTelemetry, Inc. Enters Agreement to Provide...   \n",
       "642  Factors of Influence in 2018, Key Indicators a...   \n",
       "643  New Research: Key Drivers of Growth for Micros...   \n",
       "644  Payment Data Systems Announces Apple Pay Suppo...   \n",
       "\n",
       "                                               content  \\\n",
       "0    (Bloomberg) -- Mapbox Inc., which makes mappin...   \n",
       "1    The Dow Jones rallied sharply along with the o...   \n",
       "2    The Dow Jones Industrial Average held strong g...   \n",
       "3    The Dow Jones Industrial Average held strong g...   \n",
       "4    Jonathan Boyar, Boyar Asset Management Princip...   \n",
       "..                                                 ...   \n",
       "640  NEW YORK, Jan.  31, 2018  (GLOBE NEWSWIRE) -- ...   \n",
       "641  MALVERN, Pa., Nov.  30, 2017  (GLOBE NEWSWIRE)...   \n",
       "642  NEW YORK, Nov.  27, 2017  (GLOBE NEWSWIRE) -- ...   \n",
       "643  NEW YORK, Oct.  05, 2017  (GLOBE NEWSWIRE) -- ...   \n",
       "644  SAN ANTONIO, Feb.  19, 2016  (GLOBE NEWSWIRE) ...   \n",
       "\n",
       "                                                  link  \\\n",
       "0    https://finance.yahoo.com/news/softbank-backed...   \n",
       "1    https://finance.yahoo.com/m/ae1cdff5-1fcf-3c5e...   \n",
       "2    https://finance.yahoo.com/m/fa4aff04-adc3-352a...   \n",
       "3    https://finance.yahoo.com/m/fa4aff04-adc3-352a...   \n",
       "4    https://finance.yahoo.com/video/warren-buffett...   \n",
       "..                                                 ...   \n",
       "640  https://www.globenewswire.com/news-release/201...   \n",
       "641  https://www.globenewswire.com/news-release/201...   \n",
       "642  https://www.globenewswire.com/news-release/201...   \n",
       "643  https://www.globenewswire.com/news-release/201...   \n",
       "644  https://www.globenewswire.com/news-release/201...   \n",
       "\n",
       "                                               symbols  \\\n",
       "0    ['AAPL.US', 'AMZN.US', 'AONE-UN.US', 'AONE.US'...   \n",
       "1    ['AAPL.US', 'BA.US', 'JNJ.US', 'SMG.US', 'ZM.US']   \n",
       "2    ['AAPL.US', 'BA.US', 'INTC.US', 'JNJ.US', 'TER...   \n",
       "3    ['AAPL.US', 'BA.US', 'INTC.US', 'JNJ.US', 'TER...   \n",
       "4    ['AAPL.US', 'AXP.US', 'BAC-PP.US', 'BAC.US', '...   \n",
       "..                                                 ...   \n",
       "640  ['AAPL.US', 'AVHI.US', 'FARM.US', 'GM.US', 'SG...   \n",
       "641                             ['AAPL.US', 'BEAT.US']   \n",
       "642  ['AAPL.US', 'CSCO.US', 'GD.US', 'HPE.US', 'NVD...   \n",
       "643  ['AAPL.US', 'AMZN.US', 'INTC.US', 'MSFT.US', '...   \n",
       "644                  ['AAPL.US', 'PYDS.US', 'USIO.US']   \n",
       "\n",
       "                                                  tags  \\\n",
       "0    ['BLOOMBERG', 'BY\\xa0INSTACART INC', 'CHIEF EX...   \n",
       "1    ['APPLE STOCK', 'DOW JONES', 'NASDAQ', 'NASDAQ...   \n",
       "2    ['DOW JONES', 'STOCK MARKET', 'THE DOW', 'U.S....   \n",
       "3    ['DOW JONES', 'STOCK MARKET', 'THE DOW', 'U.S....   \n",
       "4    ['BOYAR', 'JONATHAN BOYAR', 'KRISTIN MYERS', '...   \n",
       "..                                                 ...   \n",
       "640                            ['FUNDAMENTAL MARKETS']   \n",
       "641             ['BIOTELEMETRY', 'INC', 'NASDAQ:BEAT']   \n",
       "642                            ['FUNDAMENTAL MARKETS']   \n",
       "643                            ['FUNDAMENTAL MARKETS']   \n",
       "644  ['AKIMBO', 'AKIMBO CARD', 'APPLE', 'APPLE PAY'...   \n",
       "\n",
       "                                             sentiment  \\\n",
       "0    {'polarity': 0.974, 'neg': 0, 'neu': 0.934, 'p...   \n",
       "1    {'polarity': 0.494, 'neg': 0, 'neu': 0.887, 'p...   \n",
       "2    {'polarity': 0.896, 'neg': 0, 'neu': 0.59, 'po...   \n",
       "3    {'polarity': 0.896, 'neg': 0, 'neu': 0.59, 'po...   \n",
       "4    {'polarity': 0.999, 'neg': 0.04, 'neu': 0.821,...   \n",
       "..                                                 ...   \n",
       "640  {'polarity': 0.995, 'neg': 0.009, 'neu': 0.937...   \n",
       "641  {'polarity': 0.989, 'neg': 0.021, 'neu': 0.804...   \n",
       "642  {'polarity': 0.997, 'neg': 0.008, 'neu': 0.926...   \n",
       "643  {'polarity': 0.997, 'neg': 0.008, 'neu': 0.925...   \n",
       "644  {'polarity': 0.994, 'neg': 0.023, 'neu': 0.869...   \n",
       "\n",
       "                                           Basic_title  \\\n",
       "0    SoftBank-Backed Mapbox Names New CEO to Go Aft...   \n",
       "1    Dow Jones Rips 600 Points Higher, Led By Boein...   \n",
       "2    Dow Jones Rises Over 700 Points As Indexes Rec...   \n",
       "3    Dow Jones Rises Over 700 Points As Indexes Rec...   \n",
       "4    Warren Buffett signals more stock buybacks com...   \n",
       "..                                                 ...   \n",
       "640  Investor Expectations to Drive Momentum within...   \n",
       "641  BioTelemetry, Inc. Enters Agreement to Provide...   \n",
       "642  Factors of Influence in 2018, Key Indicators a...   \n",
       "643  New Research: Key Drivers of Growth for Micros...   \n",
       "644  Payment Data Systems Announces Apple Pay Suppo...   \n",
       "\n",
       "                                            Deep_title  \\\n",
       "0    softbank backed mapbox names new ceo go carmakers   \n",
       "1    dow jones rips 600 points higher led by boeing...   \n",
       "2    dow jones rises 700 points indexes recover las...   \n",
       "3    dow jones rises 700 points indexes recover las...   \n",
       "4    warren buffett signals stock buybacks coming y...   \n",
       "..                                                 ...   \n",
       "640  investor expectations drive momentum within co...   \n",
       "641  biotelemetry inc enters agreement provide card...   \n",
       "642  factors influence 2018 key indicators opportun...   \n",
       "643  new research key drivers growth microsoft appl...   \n",
       "644  payment data systems announces apple pay suppo...   \n",
       "\n",
       "                                           Spacy_title  Positive  Negative  \\\n",
       "0    {'word_list': ['SoftBank', '-', 'Backed', 'Map...  0.178975  0.087238   \n",
       "1    {'word_list': ['Dow', 'Jones', 'Rips', '600', ...  0.311447  0.477432   \n",
       "2    {'word_list': ['Dow', 'Jones', 'Rises', 'Over'...  0.112521  0.619718   \n",
       "3    {'word_list': ['Dow', 'Jones', 'Rises', 'Over'...  0.112521  0.619718   \n",
       "4    {'word_list': ['Warren', 'Buffett', 'signals',...  0.321768  0.145025   \n",
       "..                                                 ...       ...       ...   \n",
       "640  {'word_list': ['Investor', 'Expectations', 'to...  0.073805  0.005399   \n",
       "641  {'word_list': ['BioTelemetry', ',', 'Inc.', 'E...  0.657360  0.008309   \n",
       "642  {'word_list': ['Factors', 'of', 'Influence', '...  0.055207  0.009103   \n",
       "643  {'word_list': ['New', 'Research', ':', 'Key', ...  0.034298  0.011561   \n",
       "644  {'word_list': ['Payment', 'Data', 'Systems', '...  0.397003  0.104784   \n",
       "\n",
       "      Neutral  \n",
       "0    0.733787  \n",
       "1    0.211121  \n",
       "2    0.267761  \n",
       "3    0.267761  \n",
       "4    0.533206  \n",
       "..        ...  \n",
       "640  0.920796  \n",
       "641  0.334330  \n",
       "642  0.935690  \n",
       "643  0.954141  \n",
       "644  0.498213  \n",
       "\n",
       "[645 rows x 13 columns]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef589c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
