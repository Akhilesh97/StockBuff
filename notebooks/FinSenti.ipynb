{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a7aec8",
   "metadata": {},
   "source": [
    "# Financial News Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a487cea",
   "metadata": {},
   "source": [
    "### Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb61d4",
   "metadata": {},
   "source": [
    "- No need for lemmatizing/stemming\n",
    "- No need for domain stop-words\n",
    "- Used SentBert for vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c6115",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd23410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK loaded.\n",
      "Spacy loaded.\n",
      "PyTorch loaded.\n"
     ]
    }
   ],
   "source": [
    "'''Kernel Python Version 3.6.10 '''\n",
    "\n",
    "# Standard libs\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "import io\n",
    "from io import StringIO\n",
    "import inspect\n",
    "import shutil\n",
    "import ast\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import traceback\n",
    "import multiprocessing\n",
    "import requests\n",
    "import logging\n",
    "import math\n",
    "import pytz\n",
    "from itertools import chain\n",
    "from string import Template\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "import base64\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import tempfile\n",
    "import jsonschema\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple, NamedTuple, Union\n",
    "from functools import wraps\n",
    "import tqdm\n",
    "\n",
    "# graph\n",
    "import networkx as nx\n",
    "\n",
    "# Required pkgs\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import tqdm\n",
    "\n",
    "# General text correction - fit text for you (ftfy) and others\n",
    "import ftfy\n",
    "from fuzzywuzzy import fuzz\n",
    "from wordcloud import WordCloud\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE, ADASYN\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, jaccard_score, silhouette_score, homogeneity_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors, LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# scipy\n",
    "from scipy import spatial, sparse\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "from scipy.spatial.distance import euclidean, jensenshannon, cosine, cdist\n",
    "from scipy.io import mmwrite, mmread\n",
    "from scipy.stats import entropy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward, fcluster\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# sparse_dot_topn: matrix multiplier\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases, Word2Vec, KeyedVectors, FastText, LdaModel\n",
    "from gensim import utils\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import gensim.downloader as api\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "#nltk_model_data_path = \"/someppath/\"\n",
    "#nltk.data.path.append(nltk_model_data_path)\n",
    "from nltk import FreqDist, tokenize, sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "print(\"NLTK loaded.\")\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "print(\"Spacy loaded.\")\n",
    "\n",
    "# TF & Keras\n",
    "# import tensorflow as tf\n",
    "# from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import CustomObjectScope\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "# from keras.engine.topology import Layer\n",
    "# from keras import backend as K\n",
    "# from keras import initializers as initializers, regularizers, constraints, optimizers\n",
    "# from keras.layers import *\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.layers.recurrent import LSTM\n",
    "# # from keras.layers.core import Input, Dense, Activation\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "# from keras.models import Sequential, Model, load_model\n",
    "# import tensorflow_hub as hub\n",
    "# print(\"TensorFlow loaded.\")\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelWithLMHead\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "print(\"PyTorch loaded.\")\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import offline\n",
    "%matplotlib inline\n",
    "\n",
    "# Theme settings\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "sns.set_context('talk')\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.set_style(\"darkgrid\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee27d33",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "023cc754",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(\"../\")\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"models\")\n",
    "output_dir = os.path.join(root_dir, \"outputs\")\n",
    "\n",
    "## Load model files\n",
    "\n",
    "# Spacy\n",
    "spacy_model_data_path = \"../../../models/en_core_web_lg/en_core_web_lg-2.2.5\"\n",
    "nlp = spacy.load(spacy_model_data_path, disable=['ner'])\n",
    "\n",
    "# NLP Resources\n",
    "PATH_RES_DIR = \"../../../models/Resources\"\n",
    "\n",
    "# FinBert v1\n",
    "PATH_BERT_MODEL = \"../../../models/finbert_v1\"\n",
    "\n",
    "# [OLD] Distilled Roberta v1\n",
    "# PATH_BERT_MODEL = \"../../../models/all-distilroberta-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f2e22",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e621e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Twitter_Microsoft_2015.csv',\n",
       " 'Twitter_Microsoft_2016.csv',\n",
       " 'Twitter_Microsoft_2017.csv',\n",
       " 'Twitter_Microsoft_2018.csv',\n",
       " 'Twitter_Microsoft_2019.csv']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data_path = os.path.join(os.path.join(data_dir, \"datasets\"), \"news_data\")\n",
    "os.listdir(news_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3b63299",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for f in os.listdir(news_data_path):\n",
    "    data.append(pd.read_csv(os.path.join(news_data_path, f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be4223e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_errors(df, d_format):\n",
    "    dates = []\n",
    "    for i,d in enumerate(df.date.values):\n",
    "        try:\n",
    "            if \"-\" in d:\n",
    "                d_format = \"%Y-%m-%d\"\n",
    "            date = datetime.strptime(d, d_format)\n",
    "            dates.append(date)\n",
    "        except Exception as e:\n",
    "            print(i, str(e))\n",
    "            dates.append(\"NULL\")\n",
    "    df['date'] = dates\n",
    "    df = df[df['date']!=\"NULL\"].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82c6b266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2796 time data 'http://t.co/HGfsrYaRKY' does not match format '%m/%d/%y'\n",
      "6071 time data 'http://t.co/aIsdgFEzUG' does not match format '%m/%d/%y'\n",
      "7602 time data 'http://t.co/ep3toGJqiz' does not match format '%m/%d/%y'\n",
      "8530 time data 'Sin novia.....' does not match format '%m/%d/%y'\n",
      "8531 time data 'No fumo.....' does not match format '%m/%d/%y'\n",
      "8532 time data 'No bebo....' does not match format '%m/%d/%y'\n",
      "8533 time data '@Bllgates @microsoft @Xbox me regal√°is una XBOX ONE? ' does not match format '%m/%d/%y'\n",
      "8534 time data 'CON LO MAJO QUE SOY!!!!!' does not match format '%m/%d/%y'\n",
      "11337 time data ' to #Google #Android. #Micoshit' does not match format '%m/%d/%y'\n",
      "14580 time data ' https://t.co/mdbo57hwjC https://t.co/MAwpb7syAd' does not match format '%m/%d/%y'\n",
      "14701 time data ' https://t.co/b6o7agKtLM https://t.co/5R1yaYGaW8' does not match format '%m/%d/%y'\n",
      "14833 time data ' https://t.co/mdbo57hwjC https://t.co/Gay9PyP4SC' does not match format '%m/%d/%y'\n",
      "15229 time data 'https://t.co/RYR5HpeyhO' does not match format '%m/%d/%y'\n",
      "16319 time data ' https://t.co/qeY2kTnKbp https://t.co/Z2LbZoSnDF' does not match format '%m/%d/%y'\n",
      "16459 time data ' https://t.co/7ZsLYudeSv https://t.co/5jAjafJFpj' does not match format '%m/%d/%y'\n",
      "16551 time data ' https://t.co/7ZsLYudeSv https://t.co/RFF89SVp9u' does not match format '%m/%d/%y'\n",
      "17175 time data ' https://t.co/cBSR7bAkaK' does not match format '%m/%d/%y'\n",
      "17404 time data ' https://t.co/7ZsLYudeSv https://t.co/PlZk7CCRp9' does not match format '%m/%d/%y'\n",
      "17440 time data ' https://t.co/7ZsLYudeSv https://t.co/6fvy5iaeV6' does not match format '%m/%d/%y'\n",
      "18139 time data ' https://t.co/7ZsLYudeSv https://t.co/WtMTTjukOL' does not match format '%m/%d/%y'\n",
      "3866 time data 'so not true at all' does not match format '%m/%d/%Y'\n",
      "6448 time data 'https://t.co/QP8343QoLd https://t.co/Ks5EIVcyZb' does not match format '%m/%d/%Y'\n",
      "8883 time data 'Microsoft: *puts current branch update in slow ring instead*' does not match format '%m/%d/%Y'\n",
      "14363 time data 'https://t.co/67B7RghZZ3 https://t.co/xtWNQ1CfWZ' does not match format '%m/%d/%Y'\n",
      "15159 time data 'https://t.co/qjWq4MHSCV https://t.co/62EIoUlQsJ' does not match format '%m/%d/%Y'\n",
      "15449 time data 'https://t.co/4qJ15D4tqa https://t.co/46i1JdfFim' does not match format '%m/%d/%Y'\n",
      "15778 time data 'https://t.co/ZzfBG4EsYT https://t.co/cKaaCkVpsO' does not match format '%m/%d/%Y'\n",
      "16701 time data 'https://t.co/EpZECB7tN1 https://t.co/848jNo9PFG' does not match format '%m/%d/%Y'\n",
      "17959 time data 'https://t.co/w0ai04jNky https://t.co/rkwBqZDzJZ' does not match format '%m/%d/%Y'\n",
      "1625 time data 'https://t.co/dU7DxhTtFl https://t.co/yxzqVjuFLl' does not match format '%Y-%m-%d'\n",
      "2742 time data 'https://t.co/yh1G6ji6z9 https://t.co/FaN0neqpYx' does not match format '%Y-%m-%d'\n",
      "4192 time data 'https://t.co/QKGpxVFQUw https://t.co/DQOvfxsPKi' does not match format '%Y-%m-%d'\n",
      "6159 time data 'https://t.co/VL9T2rFZ63 https://t.co/sI1fJ2Fwwk' does not match format '%Y-%m-%d'\n"
     ]
    }
   ],
   "source": [
    "data[0] = remove_errors(data[0], d_format='%m/%d/%y')\n",
    "data[1] = remove_errors(data[1], d_format='%m/%d/%Y')\n",
    "data[2] = remove_errors(data[2], d_format='%m/%d/%Y')\n",
    "data[3] = remove_errors(data[3], d_format='%Y-%m-%d')\n",
    "data[4] = remove_errors(data[4], d_format='%Y-%m-%d')\n",
    "\n",
    "data = pd.concat(data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e55f72ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91298, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdb107bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>relevance_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>ÔºúCNET JapanÔºû„Ç∑„É™„Ç≥„É≥„Éê„É¨„Éº„ÇíÊ∞ó„Å´„Åô„Çã„Å™--„Éû„Ç§„ÇØ„É≠„ÇΩ„Éï„ÉàÊµÅ„ÅÆ„Çπ„Çø„Éº„Éà„Ç¢„ÉÉ„ÉóÊîØÊè¥ ...</td>\n",
       "      <td>127711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>@jtheakstone_3 ...commerce.microsoft.com and t...</td>\n",
       "      <td>37120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>[http://t.co/C5WOfESi6C] Microsoft planea un n...</td>\n",
       "      <td>155060.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>@Samuel_4000 Currently, Microsoft Account fund...</td>\n",
       "      <td>1905461.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>Microsoft has a patent, for opening a new wind...</td>\n",
       "      <td>74591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91293</th>\n",
       "      <td>2019-12-31 00:00:00</td>\n",
       "      <td>Microsoft wraps up its best year since 2009 ht...</td>\n",
       "      <td>69243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91294</th>\n",
       "      <td>2019-12-31 00:00:00</td>\n",
       "      <td>üíß As√≠ es como Amazon super√≥ a Microsoft, Apple...</td>\n",
       "      <td>44775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91295</th>\n",
       "      <td>2019-12-31 00:00:00</td>\n",
       "      <td>üß© As√≠ es como Amazon super√≥ a Microsoft, Apple...</td>\n",
       "      <td>54823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91296</th>\n",
       "      <td>2019-12-31 00:00:00</td>\n",
       "      <td>Microsoft seizes 'https://t.co/fJY4nvO9SV' fro...</td>\n",
       "      <td>2785818.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91297</th>\n",
       "      <td>2019-12-31 00:00:00</td>\n",
       "      <td>Microsoft wraps up its best year since 2009 ht...</td>\n",
       "      <td>4952475.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91298 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date                                              tweet  \\\n",
       "0      2015-01-01 00:00:00  ÔºúCNET JapanÔºû„Ç∑„É™„Ç≥„É≥„Éê„É¨„Éº„ÇíÊ∞ó„Å´„Åô„Çã„Å™--„Éû„Ç§„ÇØ„É≠„ÇΩ„Éï„ÉàÊµÅ„ÅÆ„Çπ„Çø„Éº„Éà„Ç¢„ÉÉ„ÉóÊîØÊè¥ ...   \n",
       "1      2015-01-01 00:00:00  @jtheakstone_3 ...commerce.microsoft.com and t...   \n",
       "2      2015-01-01 00:00:00  [http://t.co/C5WOfESi6C] Microsoft planea un n...   \n",
       "3      2015-01-01 00:00:00  @Samuel_4000 Currently, Microsoft Account fund...   \n",
       "4      2015-01-01 00:00:00  Microsoft has a patent, for opening a new wind...   \n",
       "...                    ...                                                ...   \n",
       "91293  2019-12-31 00:00:00  Microsoft wraps up its best year since 2009 ht...   \n",
       "91294  2019-12-31 00:00:00  üíß As√≠ es como Amazon super√≥ a Microsoft, Apple...   \n",
       "91295  2019-12-31 00:00:00  üß© As√≠ es como Amazon super√≥ a Microsoft, Apple...   \n",
       "91296  2019-12-31 00:00:00  Microsoft seizes 'https://t.co/fJY4nvO9SV' fro...   \n",
       "91297  2019-12-31 00:00:00  Microsoft wraps up its best year since 2009 ht...   \n",
       "\n",
       "       relevance_count  \n",
       "0             127711.0  \n",
       "1              37120.0  \n",
       "2             155060.0  \n",
       "3            1905461.0  \n",
       "4              74591.0  \n",
       "...                ...  \n",
       "91293          69243.0  \n",
       "91294          44775.0  \n",
       "91295          54823.0  \n",
       "91296        2785818.0  \n",
       "91297        4952475.0  \n",
       "\n",
       "[91298 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8df761ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_txt = \"tweet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214b2f1",
   "metadata": {},
   "source": [
    "## Preprocessinng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9f3d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessText:\n",
    "    \n",
    "    def __init__(self, resources_dir_path, custom_vocab=[], do_lemma=False):\n",
    "        self.stopwords_file = os.path.join(resources_dir_path, \"stopwords.txt\")\n",
    "        self.special_stopwords_file = os.path.join(resources_dir_path, \"special_stopwords.txt\")\n",
    "        self.special_characters_file = os.path.join(resources_dir_path, \"special_characters.txt\")\n",
    "        self.contractions_file = os.path.join(resources_dir_path, \"contractions.json\")\n",
    "        self.chatwords_file = os.path.join(resources_dir_path, \"chatwords.txt\")\n",
    "        self.emoticons_file = os.path.join(resources_dir_path, \"emoticons.json\")\n",
    "        self.greeting_file = os.path.join(resources_dir_path, \"greeting_words.txt\")\n",
    "        self.signature_file = os.path.join(resources_dir_path, \"signature_words.txt\")\n",
    "        self.preserve_key = \"<$>\" # preserve special vocab\n",
    "        self.vocab_list = custom_vocab\n",
    "        self.preseve = True if len(custom_vocab) > 0 else False\n",
    "        self.load_resources()\n",
    "        self.do_lemma = do_lemma\n",
    "        return\n",
    "    \n",
    "    def load_resources(self):\n",
    "        \n",
    "        ### Build Vocab Model --> Words to keep\n",
    "        self.vocab_list = set(map(str.lower, self.vocab_list))\n",
    "        self.vocab_dict = {w: self.preserve_key.join(w.split()) for w in self.vocab_list}\n",
    "        self.re_retain_words = re.compile('|'.join(sorted(map(re.escape, self.vocab_dict), key=len, reverse=True)))\n",
    "        \n",
    "        ### Build Stopwords Model --> Words to drop/delete\n",
    "        with open(self.stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords = [x.rstrip() for x in f.readlines()]\n",
    "        with open(self.special_stopwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        with open(self.special_characters_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.stopwords.extend([x.rstrip() for x in f.readlines()])\n",
    "        self.stopwords = list(sorted(set(self.stopwords).difference(self.vocab_list)))\n",
    "\n",
    "        ### Build Contractions\n",
    "        with open(self.contractions_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.contractions = dict(json.load(f))\n",
    "        \n",
    "        ### Build Chat-words\n",
    "        with open(self.chatwords_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.chat_words_map_dict, self.chat_words_list = {}, []\n",
    "            chat_words = [x.rstrip() for x in f.readlines()]\n",
    "            for line in chat_words:\n",
    "                cw = line.split(\"=\")[0]\n",
    "                cw_expanded = line.split(\"=\")[1]\n",
    "                self.chat_words_list.append(cw)\n",
    "                self.chat_words_map_dict[cw] = cw_expanded\n",
    "            self.chat_words_list = set(self.chat_words_list)\n",
    "        \n",
    "        ### Bukd social markups\n",
    "        # emoticons\n",
    "        with open(self.emoticons_file, \"r\") as f:\n",
    "            self.emoticons = re.compile(u'(' + u'|'.join(k for k in json.load(f)) + u')')\n",
    "        # emojis\n",
    "        self.emojis = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        # greeting\n",
    "        with open(self.greeting_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.greeting_words = [x.rstrip() for x in f.readlines()]\n",
    "        # signature\n",
    "        with open(self.signature_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            self.signature_words = [x.rstrip() for x in f.readlines()]\n",
    "        # spell-corrector\n",
    "        self.spell_checker = SpellChecker()   \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def reserve_keywords_from_cleaning(self, text, reset=False):\n",
    "        \"\"\" \n",
    "        Finds common words from a user-provided list of special keywords to preserve them from \n",
    "        cleaning steps. Identifies every special keyword and joins them using `self.preserve_key` during the \n",
    "        cleaning steps, and later resets it back to original word in the end.\n",
    "        \"\"\"\n",
    "        if reset is False:\n",
    "            # compile using a dict of words and their expansions, and sub them if found!\n",
    "            match_and_sub = self.re_retain_words.sub(lambda x: self.vocab_dict[x.string[x.start():x.end()]], text)\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", match_and_sub).strip()\n",
    "        else:\n",
    "            # reverse the change! - use this at the end of preprocessing\n",
    "            text = text.replace(self.preserve_key, \" \")\n",
    "            return re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", text).strip()\n",
    "\n",
    "\n",
    "    def basic_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in tqdm.tqdm(input_sentences, position=0):\n",
    "            sent = str(sent).strip()\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <‚Ä¶> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0‚Äì9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).strip()\n",
    "            cleaned_sentences.append(sent)\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def deep_clean(self, input_sentences):\n",
    "        cleaned_sentences = []\n",
    "        for sent in tqdm.tqdm(input_sentences, position=0):\n",
    "            # normalize text to \"utf-8\" encoding\n",
    "            sent = unicodedata.normalize('NFKD', str(sent)).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # lowercasing\n",
    "            sent = str(sent).strip().lower()\n",
    "\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            #\n",
    "            # *** Mark important keywords such as: Domain specific, Question words(wh-words), etc, using \n",
    "            # \"self.vocab_list\". Words from this list if found in any input sentence shall be joined using \n",
    "            # a key (self.preserve_key) during pre-processing step, and later un-joined to retain them.\n",
    "            #\n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=False)\n",
    "            #\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "\n",
    "            # remove Emojis\n",
    "            sent = self.emojis.sub(r'', sent)\n",
    "            # remove emoticons\n",
    "            sent = self.emoticons.sub(r'', sent)\n",
    "            # remove common chat-words\n",
    "            sent = \" \".join([self.chat_words_map_dict[w.upper()] if w.upper() in self.chat_words_list else w for w in sent.split()])\n",
    "            # FIX text\n",
    "            sent = ftfy.fix_text(sent)\n",
    "            # Normalize accented chars\n",
    "            sent = unicodedata.normalize('NFKD', sent).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            # Removing <‚Ä¶> web scrape tags\n",
    "            sent = re.sub(r\"\\<(.*?)\\>\", \" \", sent)\n",
    "            # Expanding contractions using contractions_file\n",
    "            sent = re.sub(r\"(\\w+\\'\\w+)\", lambda x: self.contractions.get(x.group().lower(), x.group().lower()), sent)\n",
    "            # Removing web urls\n",
    "            sent = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0‚Äì9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª\"\"'']))''', \" \", sent)\n",
    "            # Removing date formats\n",
    "            sent = re.sub(r\"(\\d{4}\\-\\d{2}\\-\\d{2}\\s\\d{2}\\:\\d{2}\\:\\d{2}\\s\\:)\", \" \", sent)\n",
    "\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "            #\n",
    "            # removing punctuations üî•üî•\n",
    "            # *** disable them, when sentence structure needs to be retained ***\n",
    "            sent = re.sub(r\"[\\$|\\#\\@\\*\\%]+\\d+[\\$|\\#\\@\\*\\%]+\", \" \", sent)\n",
    "            sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n",
    "            sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n",
    "            sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n",
    "            sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n",
    "            sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n",
    "            sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n",
    "            sent = re.sub(r\"[\\/,\\@,\\#,\\\\,\\{,\\},\\(,\\),\\[,\\],\\$,\\%,\\^,\\&,\\*,\\<,\\>]\", \" \", sent)\n",
    "            sent = re.sub(r\"[\\,,\\;,\\:,\\-]\", \" \", sent)      # main puncts\n",
    "            \n",
    "            # remove sentence de-limitters üî•üî•\n",
    "            # *** disable them, when sentence boundary/ending is important ***\n",
    "            # sent = re.sub(r\"[\\!,\\?,\\.]\", \" \", sent)\n",
    "\n",
    "            # keep only text & numbers üî•üî•\n",
    "            # *** enable them, when only text and numbers matter! *** \n",
    "            # sent = re.sub(r\"\\s+\", \" \", re.sub(r\"[\\\\|\\/|\\||\\{|\\}|\\[|\\]\\(|\\)]+\", \" \", re.sub(r\"[^A-z0-9]\", \" \", str(sent))))\n",
    "            \n",
    "            # correct spelling mistakes üî•üî•\n",
    "            # *** enable them when english spelling mistakes matter *** \n",
    "            # sent = \" \".join([self.spell_checker.correction(w) if w in self.spell_checker.unknown(sent.split()) else w for w in sent.split()])\n",
    "            #\n",
    "            # <----------------------------- OPTIONAL CLEANING ----------------------------- >\n",
    "\n",
    "            # Remove stopwords\n",
    "            sent = \" \".join(token.text for token in nlp(sent) if token.text not in self.stopwords and \n",
    "                                                                 token.lemma_ not in self.stopwords)\n",
    "            # Lemmatize\n",
    "            if self.do_lemma:\n",
    "                sent = \" \".join(token.lemma_ for token in nlp(sent))\n",
    "            # Removing extra whitespaces\n",
    "            sent = re.sub(r\"([\\s\\n\\t\\r]+)\", \" \", sent).lower().strip()\n",
    "\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "            #\n",
    "            # *** Reverse the custom joining now to un-join the special words found!\n",
    "            if self.preseve: \n",
    "                sent = self.reserve_keywords_from_cleaning(sent, reset=True)\n",
    "            # <----------------------------- CUSTOM CLEANING ----------------------------- >\n",
    "\n",
    "            cleaned_sentences.append(sent.strip().lower())\n",
    "        return cleaned_sentences\n",
    "\n",
    "\n",
    "    def spacy_get_pos_list(self, results):\n",
    "        word_list, pos_list, lemma_list, ner_list, start_end_list = [], [], [], [], []\n",
    "        indices = results['sentences']\n",
    "        for line in indices:\n",
    "            tokens = line['tokens']\n",
    "            for token in tokens:\n",
    "                # (1). save tokens\n",
    "                word_list.append(token['word'])\n",
    "                # (2). save pos\n",
    "                pos_list.append(token['pos'])\n",
    "                # (3). save lemmas\n",
    "                lemma = token['lemma'].lower()\n",
    "                if lemma in self.stopwords: continue\n",
    "                lemma_list.append(lemma)\n",
    "                # (4). save NER\n",
    "                ner_list.append(token['ner'])\n",
    "                # (5). save start\n",
    "                start_end_list.append(str(token['characterOffsetBegin']) + \"_\" + str(token['characterOffsetEnd']))\n",
    "        output = {\"word_list\": word_list, \n",
    "                  \"lemma_list\": lemma_list, \n",
    "                  \"token_start_end_list\": start_end_list,\n",
    "                  \"pos_list\": pos_list, \"ner_list\": ner_list}\n",
    "        return output\n",
    "\n",
    "    def spacy_generate_features(self, doc, operations='tokenize,ssplit,pos,lemma,ner'):\n",
    "        \"\"\"\n",
    "        Spacy nlp pipeline to generate features such as pos, tokens, ner, dependency. Accepts doc=nlp(text)\n",
    "        \"\"\"\n",
    "        # spacy doc\n",
    "        doc_json = doc.to_json()  # Includes all operations given by spacy pipeline\n",
    "\n",
    "        # Get text\n",
    "        text = doc_json['text']\n",
    "\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        # 1. Extract Entity List\n",
    "        entity_list = doc_json[\"ents\"]\n",
    "\n",
    "        # 2. Create token lib\n",
    "        token_lib = {token[\"id\"]: token for token in doc_json[\"tokens\"]}\n",
    "\n",
    "        # init output json\n",
    "        output_json = {}\n",
    "        output_json[\"sentences\"] = []\n",
    "\n",
    "        # Perform spacy operations on each sent in text\n",
    "        for i, sentence in enumerate(tqdm.tqdm(doc_json[\"sents\"], position=0)):\n",
    "            # init parsers\n",
    "            parse = \"\"\n",
    "            basicDependencies = []\n",
    "            enhancedDependencies = []\n",
    "            enhancedPlusPlusDependencies = []\n",
    "\n",
    "            # init output json\n",
    "            out_sentence = {\"index\": i, \"line\": 1, \"tokens\": []}\n",
    "            output_json[\"sentences\"].append(out_sentence)\n",
    "\n",
    "            # 3. Split sentences by indices(i), add labels (pos, ner, dep, etc.)\n",
    "            for token in doc_json[\"tokens\"]:\n",
    "\n",
    "                if sentence[\"start\"] <= token[\"start\"] and token[\"end\"] <= sentence[\"end\"]:\n",
    "                    \n",
    "                    # >>> Extract Entity label\n",
    "                    ner = \"O\"\n",
    "                    for entity in entity_list:\n",
    "                        if entity[\"start\"] <= token[\"start\"] and token[\"end\"] <= entity[\"end\"]:\n",
    "                            ner = entity[\"label\"]\n",
    "\n",
    "                    # >>> Extract dependency info\n",
    "                    dep = token[\"dep\"]\n",
    "                    governor = 0 if token[\"head\"] == token[\"id\"] else (token[\"head\"] + 1)  # CoreNLP index = pipeline index +1\n",
    "                    governorGloss = \"ROOT\" if token[\"head\"] == token[\"id\"] else text[token_lib[token[\"head\"]][\"start\"]:\n",
    "                                                                                     token_lib[token[\"head\"]][\"end\"]]\n",
    "                    dependent = token[\"id\"] + 1\n",
    "                    dependentGloss = text[token[\"start\"]:token[\"end\"]]\n",
    "\n",
    "                    # >>> Extract lemma\n",
    "                    lemma = doc[token[\"id\"]].lemma_\n",
    "\n",
    "                    # 4. Add dependencies\n",
    "                    basicDependencies.append({\"dep\": dep,\n",
    "                                              \"governor\": governor,\n",
    "                                              \"governorGloss\": governorGloss,\n",
    "                                              \"dependent\": dependent,\n",
    "                                              \"dependentGloss\": dependentGloss})\n",
    "                    # 5. Add tokens\n",
    "                    out_token = {\"index\": token[\"id\"] + 1,\n",
    "                                 \"word\": dependentGloss,\n",
    "                                 \"originalText\": dependentGloss,\n",
    "                                 \"characterOffsetBegin\": token[\"start\"],\n",
    "                                 \"characterOffsetEnd\": token[\"end\"]}\n",
    "\n",
    "                    # 6. Add lemmas\n",
    "                    if \"lemma\" in operations:\n",
    "                        out_token[\"lemma\"] = lemma\n",
    "\n",
    "                    # 7. Add POS tagging\n",
    "                    if \"pos\" in operations:\n",
    "                        out_token[\"pos\"] = token[\"tag\"]\n",
    "\n",
    "                    # 8. Add NER\n",
    "                    if \"ner\" in operations:\n",
    "                        out_token[\"ner\"] = ner\n",
    "\n",
    "                    # Update output json\n",
    "                    out_sentence[\"tokens\"].append(out_token)\n",
    "\n",
    "            # 9. Add dependencies operation\n",
    "            if \"parse\" in operations:\n",
    "                out_sentence[\"parse\"] = parse\n",
    "                out_sentence[\"basicDependencies\"] = basicDependencies\n",
    "                out_sentence[\"enhancedDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "                out_sentence[\"enhancedPlusPlusDependencies\"] = out_sentence[\"basicDependencies\"]\n",
    "        # ---------------------------------------- OPERATIONS  ---------------------------------------- #\n",
    "        return output_json\n",
    "    \n",
    "    def spacy_clean(self, input_sentences):\n",
    "        batch_size = min(int(np.ceil(len(input_sentences)/100)), 500)\n",
    "        \n",
    "        # Part 1: generate spacy textual features (pos, ner, lemma, dependencies)\n",
    "        sentences = [self.spacy_generate_features(doc) for doc in nlp.pipe(input_sentences, batch_size=batch_size, n_threads=-1)]\n",
    "        \n",
    "        # Part 2: collect all the features for each sentence\n",
    "        spacy_sentences = [self.spacy_get_pos_list(sent) for sent in sentences]\n",
    "\n",
    "        return spacy_sentences\n",
    "\n",
    "\n",
    "    ## MAIN ##\n",
    "    def run_pipeline(self, sentences, operation):\n",
    "        \"\"\"\n",
    "        Main module to execute pipeline. Accepts list of strings, and desired operation.\n",
    "        \"\"\"\n",
    "        if operation==\"\":\n",
    "            raise Exception(\"Please pass a cleaning type - `basic`, `deep` or `spacy` !!\")\n",
    "\n",
    "        # run basic cleaning\n",
    "        if \"basic\" == operation.lower(): \n",
    "            return self.basic_clean(sentences)\n",
    "\n",
    "        # run deep cleaning\n",
    "        if \"deep\" == operation.lower(): \n",
    "            return self.deep_clean(sentences)\n",
    "\n",
    "        # run spacy pipeline\n",
    "        if \"spacy\" == operation.lower(): \n",
    "            return self.spacy_clean(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee97bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CUSTOM VOCABULARY\n",
    "- List of words you wish to mark and retain them across the preprocessing steps. \n",
    "- Example, task-specific, domain-specific keywords.\n",
    "\"\"\"\n",
    "custom_vocab = [\"google\", \"goog\", \"alphabet\", \"googlee\", \"netflix\", \"netflx\", \"amazon\", \"amz\", \n",
    "                \"apple\", \"aple\", \"aws\", \"iphone\", \"mac\", \"ipad\"]\n",
    "\n",
    "\"\"\"\n",
    "LEMMATIZER\n",
    "- Truncate words to their root-known-word form, stripping off their adjectives, verbs, etc.\n",
    "- Example: \"running\" becomes \"run\", \"is\" becomes \"be\"\n",
    "\"\"\"\n",
    "do_lemmatizing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7ae905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "\n",
    "resources_dir_path = PATH_RES_DIR\n",
    "preprocessText_obj = preprocessText(resources_dir_path, custom_vocab, do_lemmatizing)\n",
    "\n",
    "def cleaning(data, text_col):\n",
    "    #data[\"Basic_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"basic\")\n",
    "    data[\"Deep_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"deep\")\n",
    "    #data[\"Spacy_%s\" % text_col] = preprocessText_obj.run_pipeline(data[text_col], \"spacy\")\n",
    "    return data\n",
    "\n",
    "\n",
    "## SAMPLE\n",
    "# df = cleaning(df, <_TEXT_COLUMN_>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e33b3d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 91298/91298 [11:41<00:00, 130.08it/s]\n"
     ]
    }
   ],
   "source": [
    "df = cleaning(data, col_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0dcf3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Vectorization + Classification using sentence-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10da9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSentiments:\n",
    "    \n",
    "    def __init__(self, model_fp):\n",
    "        # https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_fp)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_fp)\n",
    "        self.max_length = 140\n",
    "        \n",
    "    def get_sentiments(self, df_text_col):\n",
    "        \n",
    "        if isinstance(df_text_col, pd.Series):\n",
    "            df_text_col = df_text_col.tolist()\n",
    "        \n",
    "        size = len(df_text_col)\n",
    "        if size > 2000:\n",
    "            senti_neg_scores, senti_neu_scores, senti_pos_scores = [], [], []\n",
    "            try:\n",
    "                for sent in tqdm.tqdm(df_text_col, position=0):\n",
    "                    # vectorize\n",
    "                    encoded_input = self.tokenizer(sent, return_tensors='pt')\n",
    "                    # AutoModelForSequenceClassification outputs sentiments [0:Neg, 1:Neu, 2:Pos] absolute scores\n",
    "                    output = self.model(**encoded_input)\n",
    "                    # converting to labels\n",
    "                    scaled_output = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "                    senti_neg_scores.append(scaled_output[:, 0].tolist()[0])\n",
    "                    senti_neu_scores.append(scaled_output[:, 1].tolist()[0])\n",
    "                    senti_pos_scores.append(scaled_output[:, 2].tolist()[0])\n",
    "            except Exception as e:\n",
    "                pass#print(e)\n",
    "        else:\n",
    "            # vectorize\n",
    "            encoded_input = self.tokenizer(df_text_col, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "            # AutoModelForSequenceClassification outputs sentiments [0:Neg, 1:Neu, 2:Pos] absolute scores\n",
    "            output = self.model(**encoded_input)\n",
    "            # converting to labels\n",
    "            scaled_output = torch.nn.functional.softmax(output.logits, dim=-1)\n",
    "            senti_neg_scores = scaled_output[:, 0].tolist()\n",
    "            senti_neu_scores = scaled_output[:, 1].tolist()\n",
    "            senti_pos_scores = scaled_output[:, 2].tolist()\n",
    "        \n",
    "        return senti_neg_scores, senti_neu_scores, senti_pos_scores\n",
    "\n",
    "\n",
    "## SAMPLE\n",
    "# txt_col_chosen = \"<TEXT_COL>\" % col_txt\n",
    "# df['Negative'], df['Neutral'], df['Positive'] = get_sentiments(df[txt_col_chosen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7df91ad7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73050/73050 [6:01:33<00:00,  3.37it/s]      \n"
     ]
    }
   ],
   "source": [
    "# get sentiments: Neg, Neu, Pos abs scores:\n",
    "\n",
    "txt_col_chosen = \"Deep_%s\" % col_txt\n",
    "\n",
    "\n",
    "df_text_col = df[txt_col_chosen]\n",
    "senti = GenerateSentiments(PATH_BERT_MODEL)\n",
    "df['Negative'], df['Neutral'], df['Positive'] = senti.get_sentiments(df_text_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "928935ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df[['Negative', 'Neutral', 'Positive']].idxmax(axis=1)\n",
    "df['Polarity'] = np.select(\n",
    "    [df.Sentiment=='Positive', df.Sentiment=='Negative'], [1.0*df.Positive, -1.0*df.Negative], default=0.0)\n",
    "\n",
    "df.to_csv(\"../outputs/Twitter_Microsoft_Sentiments_2015_2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "283c58e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>Polarity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>2015</td>\n",
       "      <td>-0.041227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.280970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.062588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.003251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.093248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.766002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.686188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.826388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.535143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.552731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1826 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  year  Polarity_mean\n",
       "0    2015-01-01  2015      -0.041227\n",
       "1    2015-01-02  2015       0.280970\n",
       "2    2015-01-03  2015       0.062588\n",
       "3    2015-01-04  2015       0.003251\n",
       "4    2015-01-05  2015       0.093248\n",
       "...         ...   ...            ...\n",
       "1821 2019-12-27  2019       0.766002\n",
       "1822 2019-12-28  2019       0.686188\n",
       "1823 2019-12-29  2019       0.826388\n",
       "1824 2019-12-30  2019       0.535143\n",
       "1825 2019-12-31  2019       0.552731\n",
       "\n",
       "[1826 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouping by each day\n",
    "\n",
    "\n",
    "df_final = df.groupby(['date'])['Polarity'].mean().reset_index().rename(columns={'Polarity':'Polarity_mean'})\n",
    "df_final = df_final.sort_values(by=['date']).reset_index(drop=True).drop_duplicates(subset=['date'])\n",
    "df_final['date'] = pd.to_datetime(df_final['date'])\n",
    "df_final.insert(1, 'year', df_final.date.dt.year)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a607d",
   "metadata": {},
   "source": [
    "- Feb 2016 leap year"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.6",
   "language": "python",
   "name": "python_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
